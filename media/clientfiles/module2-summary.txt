Congratulations! You have completed this module. At this point, you know:  

Data pipelines move data from one place, or form, to another 

Data flows through pipelines as a series of data packets 

Latency and throughput are key design considerations for data pipelines 

Data pipeline processes include scheduling or triggering, monitoring, maintenance, and optimization 

Parallelization and I/O buffers can help mitigate bottlenecks 

Batch pipelines extract and operate on batches of data  

Batch processing applies when accuracy is critical, or the most recent data isnâ€™t required 

Streaming data pipelines ingest data packets one-by-one in rapid succession 

Streaming pipelines apply when the most current data is needed 

Examples of streaming data pipelines use cases, such as social media feeds, fraud detection, and real-time product pricing 

Modern data pipeline technologies include schema and transformation support, drag-and-drop GUIs, and security features 

Stream-processing technologies include Apache Kafka, IBM Streams, and SQLStream 